# Introduction to OpenAI

**OpenAI** is an artificial intelligence research laboratory and company known for developing  
advanced AI models, including the GPT (Generative Pre-trained Transformer) series. Their models  
power applications like natural language processing, code generation, and conversational agents.  
OpenAI provides APIs that allow developers to integrate AI capabilities into their own applications,  
making it easier to build intelligent, interactive, and automated systems. 

The **OpenAI library** is an official Python package developed by OpenAI to simplify working  
with their AI models. It lets you easily integrate powerful language models like GPT-4  
into your applications using straightforward Python code. You can send prompts, receive  
AI-generated responses, and manage API requests without complex setup.  
The library handles authentication, formatting, and parsing, so you can focus on building chatbots,  
content tools, or automation. Designed for simplicity, it makes advanced AI accessible  
even for developers new to machine learning.  


## Create a ChatGPT Platform Account

1. Open your browser and go to [https://platform.chatgpt.com](https://platform.chatgpt.com)  
   (you may be redirected to OpenAI’s developer platform).
3. Sign in with an existing account or create a new one using email, Google, or Microsoft.
4. Verify your email address if prompted.


## Add Billing and Credit

1. After logging in, open the **Billing** section from the dashboard.
2. Add a payment method (credit/debit card).
3. Purchase credits or enable pay-as-you-go billing.

> API usage requires an active billing setup.

## Create an API Key

1. In the dashboard, go to **API Keys**.
2. Click **Create new secret key**.
3. Copy the key and store it securely.

⚠️ Treat your API key like a password. Do not share it or commit it to source control.

## Install the OpenAI Client Library

Open Command Prompt or PowerShell:

```bash
pip install openai
```


## Configure Your API Key

Using *PowerShell*:

```powershell
setx OPENAI_API_KEY "your_api_key_here"
```

## Basic Python Example

This program demonstrates how to use the OpenAI Python SDK to interact with the **Responses API**,  
which is OpenAI’s unified interface for generating model outputs such as text. 

The **Responses API** is designed to replace older, separate APIs by supporting multiple output  
types (such as text or tool calls) within a single response object. In this example, only text  
output is requested, making it suitable for simple question-and-answer use cases. The API returns  
a rich response structure, but `output_text` simplifies access by concatenating all generated text  
segments into a single string.

```python
from openai import OpenAI

client = OpenAI()

response = client.responses.create(
    model="gpt-4.1-mini",
    input="Write a haiku about Python on Windows."
)

print(response.output_text)
```

This example demonstrates the simplest way to send a prompt to a ChatGPT model using the  
OpenAI Python SDK and retrieve the model’s text output. It uses the `responses.create()` method  
and the `output_text` convenience property, which automatically extracts and concatenates all text  
generated by the model into a single string. This approach is ideal for quick scripts, prototypes,  
and command-line tools where only the final text response is needed.  


## Setting API keys

If the API key is omitted, the OpenAI SDK automatically looks for it in the  
environment, typically using the `OPENAI_API_KEY` environment variable. This  
default behavior keeps sensitive credentials out of source code and enables  
safer key management across different machines, deployment targets, and  
version control systems. 

```python
from openai import OpenAI
import os

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

response = client.responses.create(
    model="gpt-4.1-mini",
    input="Write a haiku about Python on Windows."
)

print(response.output_text)
```

In this script, the API key is passed explicitly when creating the `OpenAI`  
client, which allows the SDK to authenticate requests to the OpenAI service.  
This approach makes the authentication mechanism visible in code and can be  
useful for quick tests or controlled environments, although it is generally  
not recommended for production due to security concerns.  


## Older API

This script uses the older Chat Completions API, which predates the newer  
Responses API. In this approach, conversations are created through  
`client.chat.completions.create()`, and the model output is accessed via the  
`choices` array. While this API is still supported for backward compatibility,  
it represents an earlier design that focuses only on chat-style text outputs.  

```python
from openai import OpenAI
import os

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

completion = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[
        {
          "role": "user",
          "content": "Is Pluto a planet?"
        }
    ]
)

print(completion.choices[0].message.content)
```

One reason this older API remains relevant is that many other LLM providers  
and open-source frameworks expose chat-completion–style interfaces that are  
conceptually similar. As a result, code written against this pattern can be  
easier to adapt across different models and vendors.  

By contrast, the modern Responses API unifies text generation, tool calls,  
and multimodal outputs into a single, more flexible response structure.  
Because of this broader scope, not all LLMs or platforms outside OpenAI  
support the Responses API model, making the older chat completions pattern  
more portable in some multi-model or cross-provider setups.  


## Setting roles

The **role setting** defines how each message should be interpreted by the model.    
The `system` role is used to establish high-level instructions or behavior, guiding   
the model to act as an expert in astronomy and programming. The `user` role represents  
the actual question being asked. By separating instructions (`system`) from queries (`user`),  
the model can more reliably follow constraints and produce relevant, context-aware answers. 

```python
from openai import OpenAI

client = OpenAI()

response = client.responses.create(
    model="gpt-4.1-mini",
    input=[
        {
            "role": "system",
            "content": "You are an expert in Astronomy and Programming.",
        },
        {
            "role": "user",
            "content": "Is Pluto a planet?",
        },
    ],
)


# Convenience property: concatenated plain-text output
print("--- Text Output ---")
print(response.output_text)
```

The script initializes an `OpenAI` client and sends a structured conversational input  
to the `responses.create()` method. The input is provided as a list of message objects, allowing  
the model to process context in a dialogue-like format. Once the request is processed, the  
program retrieves and prints the model’s combined plain-text output using  
the convenience property `response.output_text`.

 
## Analyze CSV data

The script reads the CSV file at `users_data.csv`, counts the data rows   
(excluding the header), and loads the full CSV text into a single natural-language prompt   
requesting a basic data analysis. It initializes an OpenAI client, constructs the prompt  
(asking for dataset structure, basic statistics, patterns, data-quality observations,  
and recommendations), then sends that prompt to the Responses API and captures the model's reply. 

```python
from openai import OpenAI
import csv

client = OpenAI()

# Read CSV file
csv_file_path = "data/users_data.csv"

# Read CSV data
with open(csv_file_path, 'r') as file:
    csv_content = file.read()

# Count rows for context
with open(csv_file_path, 'r') as file:
    csv_reader = csv.reader(file)
    row_count = sum(1 for row in csv_reader) - 1  # Subtract header row

# Prepare prompt for LLM
prompt = f"""Please analyze the following CSV dataset containing {row_count} user records.

CSV Data:
{csv_content}

Please provide:
1. A summary of the dataset structure and key columns
2. Basic statistics (e.g., age distribution, gender breakdown, country distribution)
3. Any interesting patterns or insights you notice
4. Data quality observations (missing values, outliers, etc.)
5. Recommendations for further analysis

Keep your analysis clear and concise."""

# Send to LLM for analysis
print("Analyzing CSV data with LLM...")
print("-" * 80)

response = client.responses.create(
    model="gpt-4.1-mini",
    input=prompt
)

# Print the analysis
print(response.output_text)
print("-" * 80)
print(f"\nAnalysis completed for {row_count} records from {csv_file_path}")
```

After the API call the script prints the model’s analysis and a short completion message.  
It’s a simple orchestration for quick, high-level exploratory summaries rather than exhaustive  
statistical processing—intended to run locally with configured API credentials and best used as  
a starting point for deeper analysis.


## Streaming

The script reads the CSV file at `users_data.csv`, counts the data rows (excluding the header),  
and embeds the entire CSV text into a single prompt requesting a basic data analysis  
(structure, basic statistics, patterns, data-quality observations, and recommendations). It initializes  
an OpenAI client and prepares a prompt that includes the dataset and explicit instructions for the analysis.

```python
from openai import OpenAI
import csv

client = OpenAI()

csv_file_path = "data/users_data.csv"

# Read CSV data
with open(csv_file_path, 'r') as file:
    csv_content = file.read()

# Count rows for context
with open(csv_file_path, 'r') as file:
    csv_reader = csv.reader(file)
    row_count = sum(1 for row in csv_reader) - 1  # Subtract header row

# Prepare prompt for LLM
prompt = f"""Please analyze the following CSV dataset containing {row_count} user records.

CSV Data:
{csv_content}

Please provide:
1. A summary of the dataset structure and key columns
2. Basic statistics (e.g., age distribution, gender breakdown, country distribution)
3. Any interesting patterns or insights you notice
4. Data quality observations (missing values, outliers, etc.)
5. Recommendations for further analysis

Keep your analysis clear and concise."""

print(f"Analyzing {row_count} records with streaming output...")
print("-" * 80)

# Stream the response from the model
with client.responses.stream(
    model="gpt-5-mini",
    input=prompt
) as stream:
    for event in stream:
        # Print text as it is generated
        if event.type == "response.output_text.delta":
            print(event.delta, end="", flush=True)

print("\n" + "-"*80)
print(f"Analysis completed for {row_count} records from {csv_file_path}")
```

Instead of waiting for a full response, the script opens a streaming responses  
context with `client.responses.stream` using the `gpt-5-mini` model, iterates over  
streamed events, and prints text deltas in real time as the model generates them.  
Finally, it prints a completion separator and a short message indicating the analysis  
finished for the counted records.
