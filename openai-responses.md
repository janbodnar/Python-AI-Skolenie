# Python a Ollama: pracujeme s lok√°lnymi jazykov√Ωmi modelmi  

V tomto ƒçl√°nku si uk√°≈æeme, ako sp√∫≈°≈•a≈• v√Ωkonn√© jazykov√© modely men≈°ej veƒækosti  
lok√°lne na vlastnom poƒç√≠taƒçi pomocou n√°stroja Ollama a ako ich jednoducho a  
prakticky integrova≈• do Python aplik√°ci√≠.  

## Ollama n√°stroj  

**Ollama** je v s√∫ƒçasnosti najpopul√°rnej≈°√≠ open‚Äësource n√°stroj na sp√∫≈°≈•anie,  
spr√°vu a pou≈æ√≠vanie veƒæk√Ωch jazykov√Ωch modelov lok√°lne na vlastnom poƒç√≠taƒçi.  
Umo≈æ≈àuje v√Ωvoj√°rom, v√Ωskumn√≠kom aj be≈æn√Ωm nad≈°encom pou≈æ√≠va≈• v√Ωkonn√© AI modely  
bez odovzd√°vania d√°t do cloudu, bez mesaƒçn√Ωch poplatkov a bez z√°vislosti na  
internete.  

Ollama v√Ωrazne zjednodu≈°ila cel√Ω proces pr√°ce s lok√°lnymi jazykov√Ωmi modelmi.  
Umo≈æ≈àuje jednoduch√© s≈•ahovanie a sp√∫≈°≈•anie modelov, spr√°vu VRAM/DRAM zdrojov,  
a poskytuje konzistentn√© API kompatibiln√© s OpenAI. Pon√∫ka tie≈æ jednoduch√© REST  
API a SDK pre Python a JavaScript/TypeScript.  

Pri pou≈æit√≠ Ollama z√≠skavame nasleduj√∫ce v√Ωhody:  

- Maxim√°lne s√∫kromie - ≈æiadne d√°ta neop√∫≈°≈•aj√∫ v√°≈° poƒç√≠taƒç.  
- Plne offline prev√°dzka.  
- V√Ωborn√° kompatibilita s OpenAI API.  
- Podpora volania n√°strojov, ≈°trukt√∫rovan√Ωch v√Ωstupov (JSON schema) a  
  multimod√°lnych modelov.  
- Veƒæmi jednoduch√° integr√°cia do Pythonu, JS/TS, LangChain, LlamaIndex atƒè.  

## Syst√©mov√© po≈æiadavky  

Pre efekt√≠vnu pr√°cu s Ollama je potrebn√© ma≈• primeran√Ω hardv√©r. Nasleduj√∫ca  
tabuƒæka uv√°dza odpor√∫ƒçan√© konfigur√°cie pre r√¥zne veƒækosti modelov:  

| Veƒækos≈• modelu | Minim√°lna RAM | Odpor√∫ƒçan√° RAM | GPU VRAM (4-bit) | Typick√© pou≈æitie |
|---------------|---------------|----------------|------------------|------------------|
| 1-4B | 6-8 GB | 12-16 GB | ‚Äî (staƒç√≠ CPU) | testovanie, mobiln√© zariadenia |
| 7-9B | 10-12 GB | 16-24 GB | 6-8 GB | be≈æn√° pr√°ca, programovanie |
| 13-27B | 16-24 GB | 32-48 GB | 10-16 GB | v√°≈æne pou≈æitie, RAG, anal√Ωza |
| 32-70B+ | 32+ GB | 64+ GB | 20-40+ GB | takmer GPT-4 √∫rove≈à (s GPU) |

Ollama podporuje aj CPU, no pri v√§ƒç≈°√≠ch modeloch je potrebn√© ma≈• GPU. Pre  
optim√°lny v√Ωkon sa odpor√∫ƒça ma≈• modern√∫ NVIDIA kartu s podporou CUDA a aspo≈à  
6 GB VRAM.  

## In≈°tal√°cia  

Proces in≈°tal√°cie Ollama je jednoduch√Ω. Na Linuxe m√¥≈æeme pou≈æi≈• in≈°talaƒçn√Ω  
skript alebo Docker.  

```bash
$ curl -fsSL https://ollama.com/install.sh | sh
```

Tento pr√≠kaz stiahne a nain≈°taluje Ollamu spolu s potrebn√Ωmi z√°vislos≈•ami.  

```bash
$ docker run -d --gpus all -v ollama:/johndoe/.ollama \
  -p 11434:11434 --name ollama ollama/ollama
```

Tento pr√≠kaz spust√≠ Ollama v Docker kontajneri s pr√≠stupom k GPU a  
perzistentn√Ωm √∫lo≈æiskom.  

## Z√°kladn√© CLI pr√≠kazy  

Po in≈°tal√°cii Ollama m√¥≈æete pou≈æ√≠va≈• nasleduj√∫ce pr√≠kazy v termin√°li:  

- `ollama pull gemma2:9b` - Stiahnutie modelu  
- `ollama run llama3.2:3b` - Spustenie modelu v interakt√≠vnom re≈æime  
- `ollama list` - Zoznam nain≈°talovan√Ωch modelov  
- `ollama ps` - Be≈æiace modely a spotreba zdrojov  
- `ollama stop phi4` - Zastavenie be≈æiaceho modelu  
- `ollama rm deepseek-r1:32b` - Odstr√°nenie modelu (uvoƒænenie miesta)  
- `ollama create moj-model -f Modelfile` - Vytvorenie vlastn√©ho modelu z  
  Modelfile  
- `ollama show llama3.2` - Zobrazenie detailn√Ωch inform√°ci√≠ o modeli  
- `ollama cp llama3.2 moj-zalozny-model` - Kop√≠rovanie/premenovanie modelu  
- `ollama serve` - Spustenie lok√°lneho servera  
- `ollama --help` - N√°poveda pre v≈°etky pr√≠kazy  
- `ollama --version` - Verzia Ollama  
- `ollama push meno/model` - Nahratie modelu do vlastn√©ho repozit√°ra na  
  ollama.com  
- `ollama run llama3.2 --verbose` - Spustenie modelu so zobrazen√≠m ≈°tatist√≠k  
- `ollama run llama3.2 --format json` - Vyn√∫tenie odpovede modelu vo form√°te  
  JSON  
- `ollama run llama3.2 --keepalive 1h` - Nastavenie ƒçasu, poƒças ktor√©ho ostane  
  model v pam√§ti (VRAM)  
- `ollama help run` - Podrobn√° n√°poveda pre konkr√©tny pr√≠kaz  

Tieto pr√≠kazy pokr√Ωvaj√∫ z√°kladn√∫ spr√°vu modelov, ich sp√∫≈°≈•anie a interakciu s  
Ollama prostredn√≠ctvom termin√°lu. Pr√°ca s modelmi veƒæmi pripom√≠na pr√°cu s  
kontajnermi v Dockeri, preto t√≠, ktor√≠ s√∫ s Dockerom obozn√°men√≠, sa bud√∫ c√≠ti≈•  
ako doma.  

## Najpopul√°rnej≈°ie modely v febru√°ri 2026  

Nasleduj√∫ca tabuƒæka zobrazuje najpou≈æ√≠vanej≈°ie modely, ktor√© s√∫ dostupn√© cez  
Ollama:  

| Poradie | Model | Veƒækos≈• | Siln√© str√°nky |
|---------|-------|---------|---------------|
| 1 | qwen2.5-coder | 7B-32B | programovanie, matematika, dlh√© kontexty |
| 2 | gemma3 / gemma3-it | 4B-27B | v≈°estrann√Ω v√Ωkon / cena / r√Ωchlos≈• |
| 3 | dolphin-llama3.1 | 8B-70B | agentick√© √∫lohy, tool calling |
| 4 | deepseek-r1 / deepseek-coder-v3 | 7B-67B | k√≥dovanie, matematika, reasoning |
| 5 | phi-4 / phi-4-mini | 3.8B-14B | extr√©mne r√Ωchly, dobr√Ω reasoning |
| 6 | llama3.2 / llama3.1 | 1B-70B | stabilita, dlhodobo najviac fine-tunov |
| 7 | mistral-nemo / mistral-large-3 | 12B-123B | multimod√°lny, enterprise kvalita |

V na≈°ich pr√≠kladoch budeme pou≈æ√≠va≈• model `gemma3:1b` a model  
`ministral-3:3b`.  

## Spustenie modelu Gemma 3  

Gemma 3 je otvoren√Ω jazykov√Ω model od Google DeepMind, ktor√Ω je dostupn√Ω vo  
veƒækostiach 270M, 1B, 4B, 12B a 27B parametrov. Jeho hlavn√© prednosti zah≈ï≈àaj√∫  
v√Ωborn√Ω pomer v√Ωkon/cena/r√Ωchlos≈•, ƒço z neho rob√≠ v≈°estrann√Ω model vhodn√Ω pre  
v√§ƒç≈°inu √∫loh. Napriek men≈°ej veƒækosti dosahuje prekvapivo kvalitn√© v√Ωsledky a  
pon√∫ka dobr√∫ multijazyƒçn√∫ podporu, vr√°tane slovenƒçiny. Variant 4B m√° n√≠zke  
hardv√©rov√© n√°roky a be≈æ√≠ pohodlne aj na be≈æn√Ωch poƒç√≠taƒçoch.  

```bash
$ ollama pull gemma3:1b
$ ollama list
NAME         ID              SIZE      MODIFIED       
gemma3:1b    8648f39daa8f    815 MB    10 minutes ago  
$ ollama run gemma3:1b 
>>> Is Pluto a Planet? Okay, let's break down the complex
and fascinating question of whether Pluto is still a planet. The short answer
is: **mostly, but with a significant caveat.**
...
```

Pomocou t√Ωchto pr√≠kazov si stiahneme model `gemma3:1b` a spust√≠me ho. Po  
spusten√≠ m√°me k dispoz√≠cii interakt√≠vny re≈æim, kde m√¥≈æeme kl√°s≈• ot√°zky.  

Model m√¥≈æeme pou≈æ√≠va≈• aj cez REST API, ktor√© Ollama poskytuje na porte 11434.  

```bash
$ xh :11434/api/chat model=gemma3:1b stream:=false messages:='[{"role": "user", "content": "What is the capital of Slovakia?"}]'
HTTP/1.1 200 OK
Content-Length: 367
Content-Type: application/json; charset=utf-8
Date: Mon, 26 Jan 2026 14:00:26 GMT

{
    "model": "gemma3:1b",
    "created_at": "2026-01-26T14:00:26.110078707Z",
    "message": {
        "role": "assistant",
        "content": "The capital of Slovakia is **Bratislava**. \n\nIt's a lovely city! üòä"
    },
    "done": true,
    "done_reason": "stop",
    "total_duration": 794772131,
    "load_duration": 194088768,
    "prompt_eval_count": 16,
    "prompt_eval_duration": 77271035,
    "eval_count": 21,
    "eval_duration": 510297773
}
```

Streamovanie vypneme pomocou parametra `stream:=false` a po≈°leme po≈æiadavku na  
endpoint `/api/chat`. Model n√°m vr√°ti odpoveƒè vo form√°te JSON, ktor√° obsahuje  
odpoveƒè a relevantn√© metad√°ta.  

```bash
$ xh -b :11434/api/chat model=gemma3:1b messages:='[{"role": "user", "content": "Is Pluto a planet?"}]' | jq -j '.message.content'
```

Tento pr√≠kaz vyu≈æ√≠va n√°stroj `xh` na odoslanie po≈æiadavky a `jq` na z√≠skanie  
len textovej odpovede. Voƒæba `-b` n√°stroja `xh` znamen√° "body only"; zobraz√≠ sa  
len telo odpovede (bez HTTP hlaviƒçiek). Pomocou voƒæby `-j` n√°stroja `jq` sa  
zabezpeƒç√≠, ≈æe v√Ωstup bude bez uvodzoviek a na jednom riadku.  

Takto dostaneme len ƒçist√∫ textov√∫ odpoveƒè bez JSON ≈°trukt√∫ry a form√°tovania.  

## Ofici√°lna kni≈ænica ollama  

Pre jazyk Python m√°me nat√≠vnu kni≈ænicu `ollama`, ktor√° poskytuje jednoduch√© API  
≈°peci√°lne navrhnut√© pre Ollama.  

```bash
$ uv add -U ollama
```

Kni≈ænicu si nain≈°talujeme pomocou `uv` n√°stroja. Namiesto `pip` n√°stroja sme  
pou≈æili `uv` mana≈æ√©r. V s√∫ƒçasnosti je to pre pr√°cu s modern√Ωmi AI n√°strojmi na  
Linuxe nevyhnutnos≈•.  

```python
import ollama

response = ollama.chat(
    model='gemma3:1b',
    messages=[
        {
            'role': 'system', 
            'content': 'You are a helpful assistant.'
        },
        {
            'role': 'user', 
            'content': 'Is Pluto a planet?'
        }
    ],
    options={'temperature': 0.15}
)

print(response['message']['content'])
```

V tomto pr√≠klade vytvor√≠me jednoduch√Ω chat s modelom `gemma3:1b`. Nastav√≠me  
syst√©mov√∫ spr√°vu, ktor√° definuje spr√°vanie modelu, a pou≈æ√≠vateƒæsk√∫ spr√°vu s  
konkr√©tnou po≈æiadavkou. Parameter `temperature` nastaven√Ω na 0.15 zabezpeƒç√≠  
viac deterministickej≈°ie a konzistentnej≈°ie odpovede. V√Ωsledok dostaneme cez  
slovn√≠kov√Ω `content` kƒæ√∫ƒç.  

### Streaming odpovede  

Streamovanie je u≈æitoƒçn√© pri generovan√≠ dlh≈°√≠ch textov, preto≈æe umo≈æ≈àuje  
zobrazova≈• odpoveƒè postupne poƒças jej generovania.  

```python
import ollama

stream = ollama.chat(
    model='gemma3:1b',
    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],
    stream=True,
)

for chunk in stream:
    print(chunk['message']['content'], end='', flush=True)
```

Nastaven√≠m `stream=True` dostaneme iterovan√Ω objekt, cez ktor√Ω prech√°dzame v  
cykle. Ka≈æd√Ω chunk obsahuje ƒças≈• odpovede, ktor√∫ okam≈æite vyp√≠≈°eme bez nov√©ho  
riadku (`end=''`) a s okam≈æit√Ωm vypr√°zdnen√≠m bufferu (`flush=True`), ƒço vytv√°ra  
plynul√Ω efekt p√≠sania.  

### ≈†trukt√∫rovan√© v√Ωstupy  

V ned√°vnej dobe pribudla podpora ≈°trukt√∫rovan√Ωch v√Ωstupov, ktor√© umo≈æ≈àuj√∫  
modelu generova≈• odpovede vo form√°te JSON.  

```bash
$ ollama pull ministral-3:3b
```

Keƒè≈æe ide o n√°roƒçnej≈°iu √∫lohu, pou≈æijeme v√§ƒç≈°√≠ model `ministral-3:3b`.  

```python
from ollama import chat

text = """
Extract information about people mentioned in the following text. For each
person, provide their name, age, and city of residence in a structured JSON
format. John Doe is a software engineer living in New York. He
is 30 years old and enjoys hiking and photography. Jane Smith is a graphic
designer based in San Francisco. She is 28 years old and loves painting and
traveling."""

response = chat(
  model='ministral-3:3b',
  messages=[{'role': 'user', 'content': text}],
  format='json'
)

print(response.message.content)
```

Parameter `format='json'` povie modelu aby generoval odpoveƒè vo form√°te JSON.  
Model sa pok√∫si vytvori≈• platn√∫ JSON ≈°trukt√∫ru s relevantn√Ωmi inform√°ciami  
podƒæa zadania.  

```bash
$ uv run python main.py 
{"people": [
    {
        "name": "John Doe",
        "age": 30,
        "city_of_residence": "New York"
    },
    {
        "name": "Jane Smith",
        "age": 28,
        "city_of_residence": "San Francisco"
    }
]}
```

### OpenAI-kompatibiln√© rozhranie  

Ollama poskytuje kompatibiln√© rozhranie s OpenAI API, ƒço umo≈æ≈àuje jednoduch√∫  
migr√°ciu existuj√∫ceho k√≥du.  

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11434/v1", 
    api_key="ollama" # dummy hodnota - ignoruje sa
)  

response = client.responses.create(
    model="gemma3:1b", input="Write a haiku about a gray, winter day."
)

print(response.output_text)
```

Kni≈ænica `openai` je ≈°iroko pou≈æ√≠van√° v komunite. Ollama poskytuje  
kompatibiln√Ω API endpoint (`/v1`), ƒço umo≈æ≈àuje pou≈æi≈• existuj√∫ci k√≥d urƒçen√Ω pre  
OpenAI bez v√§ƒç≈°√≠ch zmien. Staƒç√≠ upravi≈• `base_url` na lok√°lny Ollama server.  
Parameter `api_key` je ignorovan√Ω (m√¥≈æe by≈• ƒæubovoƒæn√Ω), preto≈æe lok√°lne Ollama  
nevy≈æaduje autentifik√°ciu.  

```bash
$ uv run python ollama_openai.py 
Snow falls soft and slow,
Gray light blankets all the land,
Quiet peace descends.
```

## Grounding  

Jednou z najsilnej≈°√≠ch funkci√≠ Ollama je podpora *groundingu*, ƒço umo≈æ≈àuje  
modelom pristupova≈• k extern√Ωm d√°tam a n√°strojom poƒças generovania odpoved√≠.  
Ide o sp√¥sob, ako model ‚Äûuzemni≈•" v aktu√°lnych, overiteƒæn√Ωch a extern√Ωch  
inform√°ci√°ch namiesto toho, aby sa spoliehal len na to, ƒço m√° nauƒçen√© z  
tr√©ningu.  

Jazykov√© modely s√∫ skvel√© v generovan√≠ textu, ale ich vedomosti s√∫ v≈ædy len  
tak aktu√°lne, ako d√°ta, na ktor√Ωch boli tr√©novan√©. Grounding tento probl√©m  
rie≈°i.  

Ollama vyu≈æ√≠va svoj vlastn√© properiet√°rne vyhƒæad√°vanie dostupn√© cez  
`https://ollama.com/api/web_search` a `https://ollama.com/api/web_fetch`  
endpointy.  

```bash
export OLLAMA_API_KEY="my_secret_key"
```

Pre webov√© vyhƒæad√°vanie sa potrebujeme zaregistrova≈• na `ollama.com` a z√≠ska≈•  
API kƒæ√∫ƒç. Ollama umo≈æ≈àuje registr√°ciu prostredn√≠ctvom Google alebo Github.  
N√°sledne nastav√≠me premenn√∫ prostredia `OLLAMA_API_KEY` s na≈°√≠m kƒæ√∫ƒçom.  

```python
import ollama

response = ollama.web_search("What are Vedas?", max_results=6)

for result in response.results:
    print('--- Search Result ---')
    print(f"Title: {result.title}")
    print(f"URL: {result.url}")
    print(f"Content: {result.content}\n")
    print("---------------\n")

print(f"Total Results: {len(response.results)}")
```

Funkcia `ollama.web_search` vykon√° webov√© vyhƒæad√°vanie a vr√°ti zoznam  
n√°jden√Ωch v√Ωsledkov. Poƒçet v√Ωsledkov m√¥≈æeme ≈°pecifikova≈• pomocou voƒæby  
`max_results`.  

```python
from ollama import web_fetch

result = web_fetch('https://docs.ollama.com/api/introduction')
print(result.content)
```

V pr√≠pade jedn√©ho zdroja m√¥≈æeme pou≈æi≈• funkciu `ollama.web_fetch`, ktor√°  
naƒç√≠ta obsah zadan√©ho URL a vr√°ti ho ako text.  

## Jednoduch√° anal√Ωza d√°t  

V nasleduj√∫com pr√≠klade uk√°≈æeme, ako m√¥≈æeme pou≈æi≈• Ollama pre jednoduch√∫  
anal√Ωzu d√°t. V tomto pr√≠klade budeme pracova≈• s d√°tami o pou≈æ√≠vateƒæoch, ktor√©  
s√∫ ulo≈æen√© v s√∫bore `users.csv`:  

```csv
id,first_name,last_name,email,occupation,salary,created_at
1,Jana,Nov√°kov√°,jana.novakova@gmail.com,Software Engineer,3200.0,2026-01-01
2,Peter,Kov√°ƒç,peter.kovac@example.com,Data Analyst,2800.0,2026-01-02
3,Lucia,Horv√°thov√°,lucia.horvathova@example.com,Project Manager,3500.0,2026-01-03
4,Martin,T√≥th,martin.toth@example.com,UX Designer,3000.0,2026-01-04
5,Simona,Varga,simona.varga@example.com,QA Engineer,2700.0,2026-01-05
6,Marek,Pol√°k,marek.polak@example.com,DevOps Engineer,3400.0,2026-01-06
7,Zuzana,Barto≈°ov√°,zuzana.bartosova@example.com,HR Specialist,2500.0,2026-01-07
8,Tom√°≈°,Urban,tomas.urban@example.com,Business Analyst,2900.0,2026-01-08
9,Barbora,Kr√°lov√°,barbora.kralova@simplemail.com,Marketing Manager,3300.0,2026-01-09
10,Jozef,≈†imek,jozef.simek@example.com,System Administrator,3100.0,2026-01-10
11,Michaela,Dudov√°,michaela.dudova@example.com,Content Writer,2200.0,2026-01-11
12,Richard,Bielik,richard.bielik@example.com,Product Owner,3600.0,2026-01-12
13,Katar√≠na,Farka≈°ov√°,katarina.farkasova@gmail.com,Accountant,2600.0,2026-01-13
14,Andrej,Gregor,andrej.gregor@example.com,Network Engineer,3200.0,2026-01-14
15,Veronika,Kuƒçerov√°,veronika.kucerova@gmail.com,Graphic Designer,2400.0,2026-01-15
16,Patrik,Holub,patrik.holub@gmail.com,Mobile Developer,3300.0,2026-01-16
17,Eva,≈†vecov√°,eva.svecova@example.com,Recruiter,2300.0,2026-01-17
18,Roman,Marek,roman.marek@simplemail.com,Database Administrator,3400.0,2026-01-18
19,Monika,Bla≈æekov√°,monika.blazekova@example.com,Scrum Master,3100.0,2026-01-19
20,Filip,Klein,filip.klein@example.com,Web Developer,3000.0,2026-01-20
```

V s√∫bore m√°me dvadsa≈• z√°znamov o pou≈æ√≠vateƒæoch vr√°tane ich platov. Na≈°√≠m  
cieƒæom je vygenerova≈• report obsahuj√∫ci minim√°lny, maxim√°lny, priemern√Ω plat a  
s√∫ƒçet platov.  

```python
import ollama

file_name = 'users.csv'

with open(file_name, 'r', encoding='utf-8') as file:
    data = file.read()

    prompt = f"""Generate a report containing minimum, maximum,sum, and average of salaries 
    from the CSV data provided. Please provide the results in JSON format.\n\nData:\n{data}"""

    response = ollama.chat(
        model='ministral-3:3b',
        format='json',
        messages=[
            {"role": "user", "content": prompt}
        ]
    )

    print(response['message']['content'])
```

Pr√≠klad naƒç√≠ta obsah s√∫boru `users.csv` a odosiela ho do modelu  
`ministral-3:3b` spolu s po≈æiadavkou na vygenerovanie ≈°tatistick√©ho reportu o  
platoch.  

```bash
$ uv run python data_analysis.py 
{"salary_statistics": {
    "minimum": 2200.0,
    "maximum": 3600.0,
    "sum": 48400.0,
    "average": 48400.0,
    "count": 20
    }
}
```

Model vr√°ti odpoveƒè vo form√°te JSON obsahuj√∫cu po≈æadovan√© ≈°tatistiky o platoch.  
V pr√≠pade sumy a priemeru platov sa model pom√Ωlil. Na tak√∫to √∫lohu je potrebn√©  
teda pou≈æi≈• v√§ƒç≈°√≠ model.  

