# Python a Ollama: pracujeme s lokÃ¡lnymi jazykovÃ½mi modelmi

V tomto ÄlÃ¡nku si ukÃ¡Å¾eme, ako spÃºÅ¡Å¥aÅ¥ vÃ½konnÃ© jazykovÃ© modely menÅ¡ej veÄ¾kosti
lokÃ¡lne na vlastnom poÄÃ­taÄi pomocou nÃ¡stroja Ollama a ako ich jednoducho a
prakticky integrovaÅ¥ do Python aplikÃ¡ciÃ­.

## Ollama nÃ¡stroj

**Ollama** je v sÃºÄasnosti najpopulÃ¡rnejÅ¡Ã­ openâ€‘source nÃ¡stroj na spÃºÅ¡Å¥anie,
sprÃ¡vu a pouÅ¾Ã­vanie veÄ¾kÃ½ch jazykovÃ½ch modelov lokÃ¡lne na vlastnom poÄÃ­taÄi.
UmoÅ¾Åˆuje vÃ½vojÃ¡rom, vÃ½skumnÃ­kom aj beÅ¾nÃ½m nadÅ¡encom pouÅ¾Ã­vaÅ¥ vÃ½konnÃ© AI modely
bez odovzdÃ¡vania dÃ¡t do cloudu, bez mesaÄnÃ½ch poplatkov a bez zÃ¡vislosti na
internete.

Ollama vÃ½razne zjednoduÅ¡ila celÃ½ proces prÃ¡ce s lokÃ¡lnymi jazykovÃ½mi modelmi.
UmoÅ¾Åˆuje jednoduchÃ© sÅ¥ahovanie a spÃºÅ¡Å¥anie modelov, sprÃ¡vu VRAM/DRAM zdrojov,
a poskytuje konzistentnÃ© API kompatibilnÃ© s OpenAI. PonÃºka tieÅ¾ jednoduchÃ© REST
API a SDK pre Python a JavaScript/TypeScript.

Pri pouÅ¾itÃ­ Ollama zÃ­skavame nasledujÃºce vÃ½hody:

- MaximÃ¡lne sÃºkromie - Å¾iadne dÃ¡ta neopÃºÅ¡Å¥ajÃº vÃ¡Å¡ poÄÃ­taÄ.
- Plne offline prevÃ¡dzka.
- VÃ½bornÃ¡ kompatibilita s OpenAI API.
- Podpora volania nÃ¡strojov, Å¡truktÃºrovanÃ½ch vÃ½stupov (JSON schema) a
  multimodÃ¡lnych modelov.
- VeÄ¾mi jednoduchÃ¡ integrÃ¡cia do Pythonu, JS/TS, LangChain, LlamaIndex atÄ.

## SystÃ©movÃ© poÅ¾iadavky

Pre efektÃ­vnu prÃ¡cu s Ollama je potrebnÃ© maÅ¥ primeranÃ½ hardvÃ©r. NasledujÃºca
tabuÄ¾ka uvÃ¡dza odporÃºÄanÃ© konfigurÃ¡cie pre rÃ´zne veÄ¾kosti modelov:

| VeÄ¾kosÅ¥ modelu | MinimÃ¡lna RAM | OdporÃºÄanÃ¡ RAM | GPU VRAM (4-bit) | TypickÃ© pouÅ¾itie |
|---------------|---------------|----------------|------------------|------------------|
| 1-4B | 6-8 GB | 12-16 GB | â€” (staÄÃ­ CPU) | testovanie, mobilnÃ© zariadenia |
| 7-9B | 10-12 GB | 16-24 GB | 6-8 GB | beÅ¾nÃ¡ prÃ¡ca, programovanie |
| 13-27B | 16-24 GB | 32-48 GB | 10-16 GB | vÃ¡Å¾ne pouÅ¾itie, RAG, analÃ½za |
| 32-70B+ | 32+ GB | 64+ GB | 20-40+ GB | takmer GPT-4 ÃºroveÅˆ (s GPU) |

Ollama podporuje aj CPU, no pri vÃ¤ÄÅ¡Ã­ch modeloch je potrebnÃ© maÅ¥ GPU. Pre
optimÃ¡lny vÃ½kon sa odporÃºÄa maÅ¥ modernÃº NVIDIA kartu s podporou CUDA a aspoÅˆ
6 GB VRAM.

## InÅ¡talÃ¡cia

Proces inÅ¡talÃ¡cie Ollama je jednoduchÃ½. Na Linuxe mÃ´Å¾eme pouÅ¾iÅ¥ inÅ¡talaÄnÃ½
skript alebo Docker.

```bash
$ curl -fsSL https://ollama.com/install.sh | sh
```

Tento prÃ­kaz stiahne a nainÅ¡taluje Ollamu spolu s potrebnÃ½mi zÃ¡vislosÅ¥ami.

```bash
$ docker run -d --gpus all -v ollama:/johndoe/.ollama \
  -p 11434:11434 --name ollama ollama/ollama
```

Tento prÃ­kaz spustÃ­ Ollama v Docker kontajneri s prÃ­stupom k GPU a
perzistentnÃ½m ÃºloÅ¾iskom.

## ZÃ¡kladnÃ© CLI prÃ­kazy

Po inÅ¡talÃ¡cii Ollama mÃ´Å¾ete pouÅ¾Ã­vaÅ¥ nasledujÃºce prÃ­kazy v terminÃ¡li:

- `ollama pull gemma2:9b` - Stiahnutie modelu
- `ollama run llama3.2:3b` - Spustenie modelu v interaktÃ­vnom reÅ¾ime
- `ollama list` - Zoznam nainÅ¡talovanÃ½ch modelov
- `ollama ps` - BeÅ¾iace modely a spotreba zdrojov
- `ollama stop phi4` - Zastavenie beÅ¾iaceho modelu
- `ollama rm deepseek-r1:32b` - OdstrÃ¡nenie modelu (uvoÄ¾nenie miesta)
- `ollama create moj-model -f Modelfile` - Vytvorenie vlastnÃ©ho modelu z
  Modelfile
- `ollama show llama3.2` - Zobrazenie detailnÃ½ch informÃ¡ciÃ­ o modeli
- `ollama cp llama3.2 moj-zalozny-model` - KopÃ­rovanie/premenovanie modelu
- `ollama serve` - Spustenie lokÃ¡lneho servera
- `ollama --help` - NÃ¡poveda pre vÅ¡etky prÃ­kazy
- `ollama --version` - Verzia Ollama
- `ollama push meno/model` - Nahratie modelu do vlastnÃ©ho repozitÃ¡ra na
  ollama.com
- `ollama run llama3.2 --verbose` - Spustenie modelu so zobrazenÃ­m Å¡tatistÃ­k
- `ollama run llama3.2 --format json` - VynÃºtenie odpovede modelu vo formÃ¡te
  JSON
- `ollama run llama3.2 --keepalive 1h` - Nastavenie Äasu, poÄas ktorÃ©ho ostane
  model v pamÃ¤ti (VRAM)
- `ollama help run` - PodrobnÃ¡ nÃ¡poveda pre konkrÃ©tny prÃ­kaz

Tieto prÃ­kazy pokrÃ½vajÃº zÃ¡kladnÃº sprÃ¡vu modelov, ich spÃºÅ¡Å¥anie a interakciu s
Ollama prostrednÃ­ctvom terminÃ¡lu. PrÃ¡ca s modelmi veÄ¾mi pripomÃ­na prÃ¡cu s
kontajnermi v Dockeri, preto tÃ­, ktorÃ­ sÃº s Dockerom oboznÃ¡menÃ­, sa budÃº cÃ­tiÅ¥
ako doma.

## NajpopulÃ¡rnejÅ¡ie modely v februÃ¡ri 2026

NasledujÃºca tabuÄ¾ka zobrazuje najpouÅ¾Ã­vanejÅ¡ie modely, ktorÃ© sÃº dostupnÃ© cez
Ollama:

| Poradie | Model | VeÄ¾kosÅ¥ | SilnÃ© strÃ¡nky |
|---------|-------|---------|---------------|
| 1 | qwen2.5-coder | 7B-32B | programovanie, matematika, dlhÃ© kontexty |
| 2 | gemma3 / gemma3-it | 4B-27B | vÅ¡estrannÃ½ vÃ½kon / cena / rÃ½chlosÅ¥ |
| 3 | dolphin-llama3.1 | 8B-70B | agentickÃ© Ãºlohy, tool calling |
| 4 | deepseek-r1 / deepseek-coder-v3 | 7B-67B | kÃ³dovanie, matematika, reasoning |
| 5 | phi-4 / phi-4-mini | 3.8B-14B | extrÃ©mne rÃ½chly, dobrÃ½ reasoning |
| 6 | llama3.2 / llama3.1 | 1B-70B | stabilita, dlhodobo najviac fine-tunov |
| 7 | mistral-nemo / mistral-large-3 | 12B-123B | multimodÃ¡lny, enterprise kvalita |

V naÅ¡ich prÃ­kladoch budeme pouÅ¾Ã­vaÅ¥ model `gemma3:1b` a model
`ministral-3:3b`.

## Spustenie modelu Gemma 3

Gemma 3 je otvorenÃ½ jazykovÃ½ model od Google DeepMind, ktorÃ½ je dostupnÃ½ vo
veÄ¾kostiach 270M, 1B, 4B, 12B a 27B parametrov. Jeho hlavnÃ© prednosti zahÅ•ÅˆajÃº
vÃ½bornÃ½ pomer vÃ½kon/cena/rÃ½chlosÅ¥, Äo z neho robÃ­ vÅ¡estrannÃ½ model vhodnÃ½ pre
vÃ¤ÄÅ¡inu Ãºloh. Napriek menÅ¡ej veÄ¾kosti dosahuje prekvapivo kvalitnÃ© vÃ½sledky a
ponÃºka dobrÃº multijazyÄnÃº podporu, vrÃ¡tane slovenÄiny. Variant 4B mÃ¡ nÃ­zke
hardvÃ©rovÃ© nÃ¡roky a beÅ¾Ã­ pohodlne aj na beÅ¾nÃ½ch poÄÃ­taÄoch.

```bash
$ ollama pull gemma3:1b
$ ollama list
NAME         ID              SIZE      MODIFIED       
gemma3:1b    8648f39daa8f    815 MB    10 minutes ago  
$ ollama run gemma3:1b 
>>> Is Pluto a Planet? Okay, let's break down the complex
and fascinating question of whether Pluto is still a planet. The short answer
is: **mostly, but with a significant caveat.**
...
```

Pomocou tÃ½chto prÃ­kazov si stiahneme model `gemma3:1b` a spustÃ­me ho. Po
spustenÃ­ mÃ¡me k dispozÃ­cii interaktÃ­vny reÅ¾im, kde mÃ´Å¾eme klÃ¡sÅ¥ otÃ¡zky.

Model mÃ´Å¾eme pouÅ¾Ã­vaÅ¥ aj cez REST API, ktorÃ© Ollama poskytuje na porte 11434.

```bash
$ xh :11434/api/chat model=gemma3:1b stream:=false messages:='[{"role": "user", "content": "What is the capital of Slovakia?"}]'
HTTP/1.1 200 OK
Content-Length: 367
Content-Type: application/json; charset=utf-8
Date: Mon, 26 Jan 2026 14:00:26 GMT

{
    "model": "gemma3:1b",
    "created_at": "2026-01-26T14:00:26.110078707Z",
    "message": {
        "role": "assistant",
        "content": "The capital of Slovakia is **Bratislava**. \n\nIt's a lovely city! ğŸ˜Š"
    },
    "done": true,
    "done_reason": "stop",
    "total_duration": 794772131,
    "load_duration": 194088768,
    "prompt_eval_count": 16,
    "prompt_eval_duration": 77271035,
    "eval_count": 21,
    "eval_duration": 510297773
}
```

Streamovanie vypneme pomocou parametra `stream:=false` a poÅ¡leme poÅ¾iadavku na
endpoint `/api/chat`. Model nÃ¡m vrÃ¡ti odpoveÄ vo formÃ¡te JSON, ktorÃ¡ obsahuje
odpoveÄ a relevantnÃ© metadÃ¡ta.

```bash
$ xh -b :11434/api/chat model=gemma3:1b messages:='[{"role": "user", "content": "Is Pluto a planet?"}]' | jq -j '.message.content'
```

Tento prÃ­kaz vyuÅ¾Ã­va nÃ¡stroj `xh` na odoslanie poÅ¾iadavky a `jq` na zÃ­skanie
len textovej odpovede. VoÄ¾ba `-b` nÃ¡stroja `xh` znamenÃ¡ "body only"; zobrazÃ­ sa
len telo odpovede (bez HTTP hlaviÄiek). Pomocou voÄ¾by `-j` nÃ¡stroja `jq` sa
zabezpeÄÃ­, Å¾e vÃ½stup bude bez uvodzoviek a na jednom riadku.

Takto dostaneme len ÄistÃº textovÃº odpoveÄ bez JSON Å¡truktÃºry a formÃ¡tovania.

## OficiÃ¡lna kniÅ¾nica ollama

Pre jazyk Python mÃ¡me natÃ­vnu kniÅ¾nicu `ollama`, ktorÃ¡ poskytuje jednoduchÃ© API
Å¡peciÃ¡lne navrhnutÃ© pre Ollama.

```bash
$ uv add -U ollama
```

KniÅ¾nicu si nainÅ¡talujeme pomocou `uv` nÃ¡stroja. Namiesto `pip` nÃ¡stroja sme
pouÅ¾ili `uv` manaÅ¾Ã©r. V sÃºÄasnosti je to pre prÃ¡cu s modernÃ½mi AI nÃ¡strojmi na
Linuxe nevyhnutnosÅ¥.

```python
import ollama

response = ollama.chat(
    model='gemma3:1b',
    messages=[
        {
            'role': 'system', 
            'content': 'You are a helpful assistant.'
        },
        {
            'role': 'user', 
            'content': 'Is Pluto a planet?'
        }
    ],
    options={'temperature': 0.15}
)

print(response['message']['content'])
```

V tomto prÃ­klade vytvorÃ­me jednoduchÃ½ chat s modelom `gemma3:1b`. NastavÃ­me
systÃ©movÃº sprÃ¡vu, ktorÃ¡ definuje sprÃ¡vanie modelu, a pouÅ¾Ã­vateÄ¾skÃº sprÃ¡vu s
konkrÃ©tnou poÅ¾iadavkou. Parameter `temperature` nastavenÃ½ na 0.15 zabezpeÄÃ­
viac deterministickejÅ¡ie a konzistentnejÅ¡ie odpovede. VÃ½sledok dostaneme cez
slovnÃ­kovÃ½ `content` kÄ¾ÃºÄ.

### Streaming odpovede

Streamovanie je uÅ¾itoÄnÃ© pri generovanÃ­ dlhÅ¡Ã­ch textov, pretoÅ¾e umoÅ¾Åˆuje
zobrazovaÅ¥ odpoveÄ postupne poÄas jej generovania.

```python
import ollama

stream = ollama.chat(
    model='gemma3:1b',
    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],
    stream=True,
)

for chunk in stream:
    print(chunk['message']['content'], end='', flush=True)
```

NastavenÃ­m `stream=True` dostaneme iterovanÃ½ objekt, cez ktorÃ½ prechÃ¡dzame v
cykle. KaÅ¾dÃ½ chunk obsahuje ÄasÅ¥ odpovede, ktorÃº okamÅ¾ite vypÃ­Å¡eme bez novÃ©ho
riadku (`end=''`) a s okamÅ¾itÃ½m vyprÃ¡zdnenÃ­m bufferu (`flush=True`), Äo vytvÃ¡ra
plynulÃ½ efekt pÃ­sania.

### Å truktÃºrovanÃ© vÃ½stupy

V nedÃ¡vnej dobe pribudla podpora Å¡truktÃºrovanÃ½ch vÃ½stupov, ktorÃ© umoÅ¾ÅˆujÃº
modelu generovaÅ¥ odpovede vo formÃ¡te JSON.

```bash
$ ollama pull ministral-3:3b
```

KeÄÅ¾e ide o nÃ¡roÄnejÅ¡iu Ãºlohu, pouÅ¾ijeme vÃ¤ÄÅ¡Ã­ model `ministral-3:3b`.

```python
from ollama import chat

text = """
Extract information about people mentioned in the following text. For each
person, provide their name, age, and city of residence in a structured JSON
format. John Doe is a software engineer living in New York. He
is 30 years old and enjoys hiking and photography. Jane Smith is a graphic
designer based in San Francisco. She is 28 years old and loves painting and
traveling."""

response = chat(
  model='ministral-3:3b',
  messages=[{'role': 'user', 'content': text}],
  format='json'
)

print(response.message.content)
```

Parameter `format='json'` povie modelu aby generoval odpoveÄ vo formÃ¡te JSON.
Model sa pokÃºsi vytvoriÅ¥ platnÃº JSON Å¡truktÃºru s relevantnÃ½mi informÃ¡ciami
podÄ¾a zadania.

```bash
$ uv run python main.py 
{"people": [
    {
        "name": "John Doe",
        "age": 30,
        "city_of_residence": "New York"
    },
    {
        "name": "Jane Smith",
        "age": 28,
        "city_of_residence": "San Francisco"
    }
]}
```

### OpenAI-kompatibilnÃ© rozhranie

Ollama poskytuje kompatibilnÃ© rozhranie s OpenAI API, Äo umoÅ¾Åˆuje jednoduchÃº
migrÃ¡ciu existujÃºceho kÃ³du.

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11434/v1", 
    api_key="ollama" # dummy hodnota - ignoruje sa
)  

response = client.responses.create(
    model="gemma3:1b", input="Write a haiku about a gray, winter day."
)

print(response.output_text)
```

KniÅ¾nica `openai` je Å¡iroko pouÅ¾Ã­vanÃ¡ v komunite. Ollama poskytuje
kompatibilnÃ½ API endpoint (`/v1`), Äo umoÅ¾Åˆuje pouÅ¾iÅ¥ existujÃºci kÃ³d urÄenÃ½ pre
OpenAI bez vÃ¤ÄÅ¡Ã­ch zmien. StaÄÃ­ upraviÅ¥ `base_url` na lokÃ¡lny Ollama server.
Parameter `api_key` je ignorovanÃ½ (mÃ´Å¾e byÅ¥ Ä¾ubovoÄ¾nÃ½), pretoÅ¾e lokÃ¡lne Ollama
nevyÅ¾aduje autentifikÃ¡ciu.

```bash
$ uv run python ollama_openai.py 
Snow falls soft and slow,
Gray light blankets all the land,
Quiet peace descends.
```

## Grounding

Jednou z najsilnejÅ¡Ã­ch funkciÃ­ Ollama je podpora *groundingu*, Äo umoÅ¾Åˆuje
modelom pristupovaÅ¥ k externÃ½m dÃ¡tam a nÃ¡strojom poÄas generovania odpovedÃ­.
Ide o spÃ´sob, ako model â€uzemniÅ¥" v aktuÃ¡lnych, overiteÄ¾nÃ½ch a externÃ½ch
informÃ¡ciÃ¡ch namiesto toho, aby sa spoliehal len na to, Äo mÃ¡ nauÄenÃ© z
trÃ©ningu.

JazykovÃ© modely sÃº skvelÃ© v generovanÃ­ textu, ale ich vedomosti sÃº vÅ¾dy len
tak aktuÃ¡lne, ako dÃ¡ta, na ktorÃ½ch boli trÃ©novanÃ©. Grounding tento problÃ©m
rieÅ¡i.

Ollama vyuÅ¾Ã­va svoj vlastnÃ© properietÃ¡rne vyhÄ¾adÃ¡vanie dostupnÃ© cez
`https://ollama.com/api/web_search` a `https://ollama.com/api/web_fetch`
endpointy.

```bash
export OLLAMA_API_KEY="my_secret_key"
```

Pre webovÃ© vyhÄ¾adÃ¡vanie sa potrebujeme zaregistrovaÅ¥ na `ollama.com` a zÃ­skaÅ¥
API kÄ¾ÃºÄ. Ollama umoÅ¾Åˆuje registrÃ¡ciu prostrednÃ­ctvom Google alebo Github.
NÃ¡sledne nastavÃ­me premennÃº prostredia `OLLAMA_API_KEY` s naÅ¡Ã­m kÄ¾ÃºÄom.

```python
import ollama

response = ollama.web_search("What are Vedas?", max_results=6)

for result in response.results:
    print('--- Search Result ---')
    print(f"Title: {result.title}")
    print(f"URL: {result.url}")
    print(f"Content: {result.content}\n")
    print("---------------\n")

print(f"Total Results: {len(response.results)}")
```

Funkcia `ollama.web_search` vykonÃ¡ webovÃ© vyhÄ¾adÃ¡vanie a vrÃ¡ti zoznam
nÃ¡jdenÃ½ch vÃ½sledkov. PoÄet vÃ½sledkov mÃ´Å¾eme Å¡pecifikovaÅ¥ pomocou voÄ¾by
`max_results`.

```python
from ollama import web_fetch

result = web_fetch('https://docs.ollama.com/api/introduction')
print(result.content)
```

V prÃ­pade jednÃ©ho zdroja mÃ´Å¾eme pouÅ¾iÅ¥ funkciu `ollama.web_fetch`, ktorÃ¡
naÄÃ­ta obsah zadanÃ©ho URL a vrÃ¡ti ho ako text.

## JednoduchÃ¡ analÃ½za dÃ¡t

V nasledujÃºcom prÃ­klade ukÃ¡Å¾eme, ako mÃ´Å¾eme pouÅ¾iÅ¥ Ollama pre jednoduchÃº
analÃ½zu dÃ¡t. V tomto prÃ­klade budeme pracovaÅ¥ s dÃ¡tami o pouÅ¾Ã­vateÄ¾och, ktorÃ©
sÃº uloÅ¾enÃ© v sÃºbore `users.csv`:

```csv
id,first_name,last_name,email,occupation,salary,created_at
1,Jana,NovÃ¡kovÃ¡,jana.novakova@gmail.com,Software Engineer,3200.0,2026-01-01
2,Peter,KovÃ¡Ä,peter.kovac@example.com,Data Analyst,2800.0,2026-01-02
3,Lucia,HorvÃ¡thovÃ¡,lucia.horvathova@example.com,Project Manager,3500.0,2026-01-03
4,Martin,TÃ³th,martin.toth@example.com,UX Designer,3000.0,2026-01-04
5,Simona,Varga,simona.varga@example.com,QA Engineer,2700.0,2026-01-05
6,Marek,PolÃ¡k,marek.polak@example.com,DevOps Engineer,3400.0,2026-01-06
7,Zuzana,BartoÅ¡ovÃ¡,zuzana.bartosova@example.com,HR Specialist,2500.0,2026-01-07
8,TomÃ¡Å¡,Urban,tomas.urban@example.com,Business Analyst,2900.0,2026-01-08
9,Barbora,KrÃ¡lovÃ¡,barbora.kralova@simplemail.com,Marketing Manager,3300.0,2026-01-09
10,Jozef,Å imek,jozef.simek@example.com,System Administrator,3100.0,2026-01-10
11,Michaela,DudovÃ¡,michaela.dudova@example.com,Content Writer,2200.0,2026-01-11
12,Richard,Bielik,richard.bielik@example.com,Product Owner,3600.0,2026-01-12
13,KatarÃ­na,FarkaÅ¡ovÃ¡,katarina.farkasova@gmail.com,Accountant,2600.0,2026-01-13
14,Andrej,Gregor,andrej.gregor@example.com,Network Engineer,3200.0,2026-01-14
15,Veronika,KuÄerovÃ¡,veronika.kucerova@gmail.com,Graphic Designer,2400.0,2026-01-15
16,Patrik,Holub,patrik.holub@gmail.com,Mobile Developer,3300.0,2026-01-16
17,Eva,Å vecovÃ¡,eva.svecova@example.com,Recruiter,2300.0,2026-01-17
18,Roman,Marek,roman.marek@simplemail.com,Database Administrator,3400.0,2026-01-18
19,Monika,BlaÅ¾ekovÃ¡,monika.blazekova@example.com,Scrum Master,3100.0,2026-01-19
20,Filip,Klein,filip.klein@example.com,Web Developer,3000.0,2026-01-20
```

V sÃºbore mÃ¡me dvadsaÅ¥ zÃ¡znamov o pouÅ¾Ã­vateÄ¾och vrÃ¡tane ich platov. NaÅ¡Ã­m
cieÄ¾om je vygenerovaÅ¥ report obsahujÃºci minimÃ¡lny, maximÃ¡lny, priemernÃ½ plat a
sÃºÄet platov.

```python
import ollama

file_name = 'users.csv'

with open(file_name, 'r', encoding='utf-8') as file:
    data = file.read()

    prompt = f"""Generate a report containing minimum, maximum,sum, and average of salaries 
    from the CSV data provided. Please provide the results in JSON format.\n\nData:\n{data}"""

    response = ollama.chat(
        model='ministral-3:3b',
        format='json',
        messages=[
            {"role": "user", "content": prompt}
        ]
    )

    print(response['message']['content'])
```

PrÃ­klad naÄÃ­ta obsah sÃºboru `users.csv` a odosiela ho do modelu
`ministral-3:3b` spolu s poÅ¾iadavkou na vygenerovanie Å¡tatistickÃ©ho reportu o
platoch.

```bash
$ uv run python data_analysis.py 
{"salary_statistics": {
    "minimum": 2200.0,
    "maximum": 3600.0,
    "sum": 48400.0,
    "average": 48400.0,
    "count": 20
    }
}
```

Model vrÃ¡ti odpoveÄ vo formÃ¡te JSON obsahujÃºcu poÅ¾adovanÃ© Å¡tatistiky o platoch.
V prÃ­pade sumy a priemeru platov sa model pomÃ½lil. Na takÃºto Ãºlohu je potrebnÃ©
teda pouÅ¾iÅ¥ vÃ¤ÄÅ¡Ã­ model.

VÅ¡etky prÃ­klady z ÄlÃ¡nku a mnohÃ© ÄalÅ¡ie sÃº dostupnÃ© na GitHub repozitÃ¡ri
[github.com/janbodnar/Python-AI-Skolenie](https://github.com/janbodnar/Python-AI-Skolenie).

V nasledujÃºcich ÄlÃ¡nkoch by som sa chcel venovaÅ¥ pokroÄilejÅ¡Ã­m tÃ©mam, ako je
tvorba agentov, fine tuning modelov, tvorba RAG systÃ©mu, MCP serverov, alebo
hlbÅ¡ej prÃ¡ci s dokumentmi.

---

> **PoznÃ¡mka:** PÃ´vodnÃ½ HTML obsah obsahoval JavaScript pre tlaÄidlÃ¡ "Copy to
> clipboard" a CSS Å¡tÃ½ly pre tmavÃ½/svetlÃ½ reÅ¾im. Tieto interaktÃ­vne a vizuÃ¡lne
> prvky neboli do Markdown formÃ¡tu prevedenÃ©, keÄÅ¾e Markdown nepodporuje priame
> vkladanie JavaScriptu ani komplexnÃ©ho CSS. Pre zachovanie tÃ½chto funkciÃ­ by
> bolo potrebnÃ© pouÅ¾iÅ¥ rozÅ¡Ã­renÃ½ Markdown renderer alebo pridaÅ¥ tieto prvky
> manuÃ¡lne po konverzii.
